Web Crawler: This is a kind of robot which crawls/processes web pages and the links on it.
The result set of web crawlers are used by search engines and different comparing sites(Price comparing sites) to render results.

Links Referred: https://medium.com/double-pointer/top-5-videos-for-web-crawler-system-design-interview-75b7ac9c04ce
https://www.youtube.com/watch?v=BKZxZwUgL3Y
https://www.youtube.com/watch?v=B994RSzlCww

                                                      Features
1)Crawling Frequency : This is the rate at which a web page should be visited. We can also set proiority of web pages i.e which web page should be often crawled.
2)Dedup : We need to avoid visting dupliacte urls.
3)Capacity: Each page that is crawled will carry several URLs to index. Assume an estimate of around 50 billion pages. Assuming an average page size of 100kb:
50 B x 100 KBytes = 5 petabytes
You would need around 5 petabytes of storage, give or take 2 petabytes, to hold the information on the web.

                                                    Design Diagram
URL Frontier(Seed Urls)-----URL----->Fetcher------Request to get IP-----> DNS
                                       |
                                       |
                                      Website---->Cache(will cache website content)-------------------------------
                                                    ^                                                            |
                                                    |                                                            |
                                                    |                                                            |
                                                    |                                                            |
If not duplicate url send to FRONtier---------------------------------(Crawled URL Set)URL DEDUP<----URL Filter<---URL Extractor             Content Seen----------(DB which will hold docs checksum)
                                                                                                                                                  |
                                                                                                                                                  |
                                                                                                                                            Storage(S3/HDFS)
                                      
 1)Seed URLS: These are initial urls on which web crawler will start cralwing. eg: for a flight comparison site, I can initally add some travel site urls.
 2)URL Frontier: This component provide a url to crawl to URL fetcher.
 3)URL Fetcher: It requests a URL form URL Frontier and after that it sends requets to DNS to get IP and then make call to web server and render its content.
 4)This content is cached (Redis) for some time, so that other processing components like URL Extractor and Content Seen can get data form it in less time.
 Since in Persistent storage there will be large volume fo data and query will slow.
 5)Content Seen Module : It will get web page from cache , process it and checks if the checksum of web page is alreay there in db, discard its processing.
 Else saves in DB. S3/HDFS. Data from this will be used by website/ search engine.
 6)URL Extractor: This will get web page from cache, extracts URLs . Now apply normalization on URL. i.e some URL clean up eg : http://abc.com and https://abc.com?a=b
 are same
 7)URL Filtering: This can filter a URL based on domain eg: we can filter all websites from .com
 Secondly, it can do robot exclusion. 
 Robot Exclusion: websites can maintain robot.txt at top level of webpage hierarchy and list what all URLS are not allowed to visit by a robot.
 8)URL Dedup: Now we will check if we have already visited/crawled this URL. If so, we wont send it to URL FRONTIER else send. We can maintain a set of URLS.
In case of continous cralwing , same page may be crawled , in that case we will always add it to URL Frontier but its priority may get change.

----------------------------------------------------------------URL Frontier-----------------------------------------

This is the main component which is giving us URL.
There are two considerations:
a)Priority: i.e we should be able to set priority of URLS.
b)Politeness: same domain requests should not hit back to back as it may down web server.

1)Priortizer: This assign priortiy to a URL and based on its priority send it to one of Front queue.
2)Front Queues: There are 1 to n queues. Each queue has certain priority value assigned.
3)Biased Front Queue Selector: When back queues request for a URL, it picks URL from highest priority non empty queue.
4)Back queue: These queus are mapped with domais i.e same domain urls are sent to same queue. And this assginment of queue-domain keeps on changing when certain domain URLS 
are finished, then this queue may assing to some other domain.
5)Back queue selctor: This pick URL from a domian queue and send it to min heap.
6)Min Heap: This maintains heap of back queues. so suppose at 1PM, Back queue1 of min heap is returned, then after its processing is completed by 1:15, this will be added
back to min heap.

*************************************************************Scaling************************************************
1) In case of scaling, we can increase the no of content seen and URL extractor processor.
2) We can implement caching at DNS resolver as resolving an DN to IP usually takes times.
3) All URL fetchers can have their own DNS resolver.
4)We can also have a host splitter which will based on domain(geography), send it to individual URL Frontier.

 
                                      
                                      
